{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0429 12:25:55.234136 140736563217344 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "#Please use python 3.5 or above\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, Concatenate, Bidirectional, Conv2D, TimeDistributed, Add, Conv1D, Reshape, Flatten, Dropout, MaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.models import load_model, Model\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return s in emoji.UNICODE_EMOJI\n",
    "\n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char + ' ' if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "def remove_text(text):\n",
    "    words = text.split(' ')\n",
    "    emojis = ' '.join([word for word in words if is_emoji(word)])\n",
    "    return emojis\n",
    "\n",
    "def count_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_upper_case(text):\n",
    "    return sum(1 for c in text if c.isupper())\n",
    "\n",
    "def str2emoji(text):\n",
    "    text = re.sub(r\":‑\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\)\\)\\)\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\)\\)\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\)\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"\\(:\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":-\\]\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\]\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":-3\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":3\",\"  ☺️ \",text)\n",
    "    text = re.sub(r\":->\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"</3\",' 💔 ',text)\n",
    "    text = re.sub(r\"<3\",\" ❤️ \",text)\n",
    "    text = re.sub(r\":>\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"8-\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":o\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":-\\}\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\}\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":-\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":c\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\^\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"=\\]\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"=\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":‑D\",\" 😃 \",text)\n",
    "    text = re.sub(r\":D\",\" 😃 \",text)\n",
    "    text = re.sub(r\"8‑D\",\" 😃 \",text)\n",
    "    text = re.sub(r\"8D\",\" 😃 \",text)\n",
    "    text = re.sub(r\"X‑D\",\" 😃 \",text)\n",
    "    text = re.sub(r\"xD\",\" 😃 \",text)\n",
    "    text = re.sub(r\"xd\",\" 😃 \",text)\n",
    "    text = re.sub(r\"XD\",\" 😃 \",text)\n",
    "    text = re.sub(r\"=D\",\" 😃 \",text)\n",
    "    text = re.sub(r\"=3\",\" 😃 \",text)\n",
    "    text = re.sub(r\"B\\^D\",\" 😃 \",text)\n",
    "    text = re.sub(r\":-\\)\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\":‑\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":-\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":-d\", \" 🤤 \",text)\n",
    "    text = re.sub(r\":d\", \" 🤤 \",text)\n",
    "    text = re.sub(r\"=d\", \" 🤤 \",text)\n",
    "\n",
    "    text = re.sub(r\":\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":‑c\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":c\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":‑<\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":<\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":‑\\[\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":\\[\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":-\\|\\|\",\" ☹️ \",text)\n",
    "    text = re.sub(r\">:\\[\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":\\{\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":@\",\" ☹️ \",text)\n",
    "    text = re.sub(r\">:\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":'‑\\(\",\" 😭 \",text)\n",
    "    text = re.sub(r\":'\\(\",\" 😭 \",text)\n",
    "    text = re.sub(r\":'‑\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\":'\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"D‑':\",\" 😧 \",text)\n",
    "    text = re.sub(r\"D:<\",\" 😨 \",text)\n",
    "    text = re.sub(r\"D:\",\" 😧 \",text)\n",
    "    text = re.sub(r\"D8\",\" 😧 \",text)\n",
    "    text = re.sub(r\"D;\",\" 😧 \",text)\n",
    "    text = re.sub(r\"D=\",\" 😧 \",text)\n",
    "    text = re.sub(r\"DX\",\" 😧 \",text)\n",
    "    text = re.sub(r\":‑O\",\" 😮 \",text)\n",
    "    text = re.sub(r\":O\",\" 😮 \",text)\n",
    "    text = re.sub(r\":‑o\",\" 😮 \",text)\n",
    "    text = re.sub(r\":o\",\" 😮 \",text)\n",
    "    text = re.sub(r\":-0\",\" 😮 \",text)\n",
    "    text = re.sub(r\"8‑0\",\" 😮 \",text)\n",
    "    text = re.sub(r\">:O\",\" 😮 \",text)\n",
    "    text = re.sub(r\":-\\*\",\" 😗 \",text)\n",
    "    text = re.sub(r\":\\*\",\" 😗 \",text)\n",
    "    text = re.sub(r\":X\",\" 😗 \",text)\n",
    "    text = re.sub(r\";‑\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\";\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\"\\*-\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\"\\*\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\";‑\\]\",\" 😉 \",text)\n",
    "    text = re.sub(r\";\\]\",\" 😉 \",text)\n",
    "    text = re.sub(r\";\\^\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\":‑,\",\" 😉 \",text)\n",
    "    text = re.sub(r\";D\",\" 😉 \",text)\n",
    "    text = re.sub(r\":‑P\",\" 😛 \",text)\n",
    "    text = re.sub(r\":‑p\",\" 😛 \",text)\n",
    "    text = re.sub(r\":P\",\" 😛 \",text)\n",
    "    text = re.sub(r\":p\",\" 😛 \",text)\n",
    "    text = re.sub(r\"X‑P\",\" 😛 \",text)\n",
    "    text = re.sub(r\":‑Þ\",\" 😛 \",text)\n",
    "    text = re.sub(r\":Þ\",\" 😛 \",text)\n",
    "    text = re.sub(r\":b\",\" 😛 \",text)\n",
    "    text = re.sub(r\"d:\",\" 😛 \",text)\n",
    "    text = re.sub(r\"=p\",\" 😛 \",text)\n",
    "    text = re.sub(r\">:P\",\" 😛 \",text)\n",
    "    text = re.sub(r\":‑/\",\" 😕 \",text)\n",
    "    text = re.sub(r\":/\",\" 😕 \",text)\n",
    "    text = re.sub(r\":-\\[\\.\\]\",\" 😕 \",text)\n",
    "    text = re.sub(r\">:/\",\" 😕 \",text)\n",
    "    text = re.sub(r\"=/\",\" 😕 \",text)\n",
    "    text = re.sub(r\":L\",\" 😕 \",text)\n",
    "    text = re.sub(r\"=L\",\" 😕 \",text)\n",
    "    text = re.sub(r\":S\",\" 😕 \",text)\n",
    "    text = re.sub(r\":‑\\|\",\" 😐 \",text)\n",
    "    text = re.sub(r\":\\|\",\" 😐 \",text)\n",
    "    text = re.sub(r\":$\",\" 😳 \",text)\n",
    "    text = re.sub(r\":‑x\",\" 🤐 \",text)\n",
    "    text = re.sub(r\":x\",\" 🤐 \",text)\n",
    "    text = re.sub(r\":‑#\",\" 🤐 \",text)\n",
    "    text = re.sub(r\":#\",\" 🤐 \",text)\n",
    "    text = re.sub(r\":‑&\",\" 🤐 \",text)\n",
    "    text = re.sub(r\":&\",\" 🤐 \",text)\n",
    "    text = re.sub(r\"O:‑\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"O:\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0:‑3\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0:3\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0:‑\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0:\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\":‑b\",\" 😛 \",text)\n",
    "    text = re.sub(r\"0;\\^\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\">:‑\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\">:\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"\\}:‑\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"\\}:\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"3:‑\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"3:\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\">;\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"\\|;‑\\)\",\" 😎 \",text)\n",
    "    text = re.sub(r\"\\|‑O\",\" 😏 \",text)\n",
    "    text = re.sub(r\":‑J\",\" 😏 \",text)\n",
    "    text = re.sub(r\"%‑\\)\",\" 😵 \",text)\n",
    "    text = re.sub(r\"%\\)\",\" 😵 \",text)\n",
    "    text = re.sub(r\":-###..\",\" 🤒 \",text)\n",
    "    text = re.sub(r\":###..\",\" 🤒 \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)\",\" 😣 \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)>\",\" 😣 \",text)\n",
    "    text = re.sub(r\"\\(';'\\)\",\" 👶 \",text)\n",
    "    text = re.sub(r\"\\(\\^\\^>``\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^;\\)\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(-_-;\\)\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(~_~;\\) \\(・\\.・;\\)\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)zzz\",\" 😴 \",text)\n",
    "    text = re.sub(r\"\\(\\^_-\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\"\\(\\(\\+_\\+\\)\\)\",\" 😕 \",text)\n",
    "    text = re.sub(r\"\\(\\+o\\+\\)\",\" 😕 \",text)\n",
    "    text = re.sub(r\"\\^_\\^\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\)/\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^O\\^\\)／\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^o\\^\\)／\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(__\\)\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"_\\(\\._\\.\\)_\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"<\\(_ _\\)>\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"<m\\(__\\)m>\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"m\\(__\\)m\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"m\\(_ _\\)m\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"\\('_'\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(/_;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(T_T\\) \\(;_;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(;_;\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(;_:\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(;O;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(:_;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(ToT\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\";_;\",\" 😭 \",text)\n",
    "    text = re.sub(r\";-;\",\" 😭 \",text)\n",
    "    text = re.sub(r\";n;\",\" 😭 \",text)\n",
    "    text = re.sub(r\";;\",\" 😭 \",text)\n",
    "    text = re.sub(r\"Q\\.Q\",\" 😭 \",text)\n",
    "    text = re.sub(r\"T\\.T\",\" 😭 \",text)\n",
    "    text = re.sub(r\"QQ\",\" 😭 \",text)\n",
    "    text = re.sub(r\"Q_Q\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(-\\.-\\)\",\" 😞 \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)\",\" 😞 \",text)\n",
    "    text = re.sub(r\"-_-\",\" 😞 \",text)\n",
    "    text = re.sub(r\"\\(一一\\)\",\" 😞 \",text)\n",
    "    text = re.sub(r\"\\(；一_一\\)\",\" 😞 \",text)\n",
    "    text = re.sub(r\"\\(=_=\\)\",\" 😩 \",text)\n",
    "    text = re.sub(r\"\\(=\\^\\·\\^=\\)\",\" 😺 \",text)\n",
    "    text = re.sub(r\"\\(=\\^\\·\\·\\^=\\)\",\" 😺 \",text)\n",
    "    text = re.sub(r\"=_\\^= \",\" 😺 \",text)\n",
    "    text = re.sub(r\"\\(\\.\\.\\)\",\" 😔 \",text)\n",
    "    text = re.sub(r\"\\(\\._\\.\\)\",\" 😔 \",text)\n",
    "    text = re.sub(r\"\\(\\・\\・\\?\",\" 😕 \",text)\n",
    "    text = re.sub(r\"\\(\\?_\\?\\)\",\" 😕 \",text)\n",
    "    text = re.sub(r\">\\^_\\^<\",\" 😃 \",text)\n",
    "    text = re.sub(r\"<\\^!\\^>\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\^/\\^\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\（\\*\\^_\\^\\*）\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(^\\^\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^\\.\\^\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\.\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^\\^\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^J\\^\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\*\\^\\.\\^\\*\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(\\^—\\^\\）\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(#\\^\\.\\^#\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\（\\^—\\^\\）\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(;_;\\)/~~~\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(\\^\\.\\^\\)/~~~\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(T_T\\)/~~~\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(\\*\\^0\\^\\*\\)\",\" 😍 \",text)\n",
    "    text = re.sub(r\"\\(\\*_\\*\\)\",\" 😍 \",text)\n",
    "    text = re.sub(r\"\\(\\*_\\*;\",\" 😍 \",text)\n",
    "    text = re.sub(r\"\\(\\+_\\+\\) \\(@_@\\)\",\" 😍 \",text)\n",
    "    text = re.sub(r\"\\(\\*\\^\\^\\)v\",\" 😂 \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\)v\",\" 😂 \",text)\n",
    "    text = re.sub(r\"\\(ーー;\\)\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(\\^0_0\\^\\)\",\" 😎 \",text)\n",
    "    text = re.sub(r\"\\(\\＾ｖ\\＾\\)\",\" 😀 \",text)\n",
    "    text = re.sub(r\"\\(\\＾ｕ\\＾\\)\",\" 😀 \",text)\n",
    "    text = re.sub(r\"\\(\\^\\)o\\(\\^\\)\",\" 😀 \",text)\n",
    "    text = re.sub(r\"\\(\\^O\\^\\)\",\" 😀 \",text)\n",
    "    text = re.sub(r\"\\(\\^o\\^\\)\",\" 😀 \",text)\n",
    "    text = re.sub(r\"\\)\\^o\\^\\(\",\" 😀 \",text)\n",
    "    text = re.sub(r\":O o_O\",\" 😮 \",text)\n",
    "    text = re.sub(r\"o_0\",\" 😮 \",text)\n",
    "    text = re.sub(r\"o\\.O\",\" 😮 \",text)\n",
    "    text = re.sub(r\"\\(o\\.o\\)\",\" 😮 \",text)\n",
    "    text = re.sub(r\"oO\",\" 😮 \",text)\n",
    "    text = re.sub(r':\\‑\\)','😃',text)\n",
    "    text = re.sub(r\":\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":-]\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":]\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"8\\-\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":o\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":-}\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":}\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":\\-\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":c\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\":^\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"=]\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"=\\)\",\" ☺️ \",text)\n",
    "    text = re.sub(r\"B^D\",\" 😃 \",text)\n",
    "    text = re.sub(r\":-\\)\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\":-\\(\",\" ☹️ \",text)\n",
    "\n",
    "    text = re.sub(r\":‑\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":\\‑\\[\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":\\[\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":-\\|\\|\",\" ☹️ \",text)\n",
    "    text = re.sub(r\">\\:\\[\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":{\",\" ☹️ \",text)\n",
    "    text = re.sub(r\">\\:\\(\",\" ☹️ \",text)\n",
    "    text = re.sub(r\":'‑\\(\",\" 😭 \",text)\n",
    "    text = re.sub(r\":'\\(\",\" 😭 \",text)\n",
    "    text = re.sub(r\":'\\‑\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\":'\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\":-\\*\",\" 😗 \",text)\n",
    "    text = re.sub(r\":\\*\",\" 😗 \",text)\n",
    "    text = re.sub(r\";\\‑\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\";\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\"\\*\\-\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\"\\*\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\";‑\\]\",\" 😉 \",text)\n",
    "    text = re.sub(r\";\\]\",\" 😉 \",text)\n",
    "    text = re.sub(r\";^\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\">\\:\\[\\(\\)\\]\",\" 😕 \",text)\n",
    "\n",
    "    text = re.sub(r\":\\[\\(\\)\\]\",\" 😕 \",text)\n",
    "    text = re.sub(r\"=\\[\\(\\)\\]\",\" 😕 \",text)\n",
    "    text = re.sub(r\":‑\\|\",\" 😐 \",text)\n",
    "    text = re.sub(r\":\\|\",\" 😐 \",text)\n",
    "    text = re.sub(r\"O:‑\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"O:\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0:‑\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0:\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\"0;^\\)\",\" 😇 \",text)\n",
    "    text = re.sub(r\">:‑\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\">:\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"}:‑\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"}:\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"3:‑\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"3:\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\">;\\)\",\" 😈 \",text)\n",
    "    text = re.sub(r\"\\|;‑\\)\",\" 😎 \",text)\n",
    "    text = re.sub(r\"\\|‑O\",\" 😏 \",text)\n",
    "    text = re.sub(r\"%‑\\)\",\" 😵 \",text)\n",
    "    text = re.sub(r\"%\\)\",\" 😵 \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)\",\" 😣 \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)>\",\" 😣 \",text)\n",
    "    text = re.sub(r\"\\(';'\\)\",\" Baby \",text)\n",
    "    text = re.sub(r\"\\(^^>``\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(^_^;\\)\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(-_-;\\)\",\" 😓 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(~_~;\\) \\(・\\.・;\\)\",\" 😓 \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)zzz\",\" 😴 \",text)\n",
    "    text = re.sub(r\"\\(^_-\\)\",\" 😉 \",text)\n",
    "    text = re.sub(r\"\\(\\(\\+_\\+\\)\\)\",\" 😕 \",text)\n",
    "    text = re.sub(r\"\\(\\+o\\+\\)\",\" 😕 \",text)\n",
    "    text = re.sub(r\"^_^\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(^_^\\)/\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(^O^\\)／\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(__\\)\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"_\\(._.\\)_\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"<\\(_ _\\)>\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"<m\\(__\\)m>\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"m\\(__\\)m\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"m\\(_ _\\)m\",\" 🙇 \",text)\n",
    "    text = re.sub(r\"\\('_'\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(/_;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(T_T\\) (;_;)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(;_;\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(;_:\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(;O;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(:_;\\)\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(ToT\\)\",\"  😭  \",text)\n",
    "    text = re.sub(r\"Q\\.Q\",\" 😭 \",text)\n",
    "    text = re.sub(r\"T\\.T\",\" 😭 \",text)\n",
    "    text = re.sub(r\"\\(-\\.-\\)\",\" 😞 \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)\",\" 😞 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(一一\\)\",\" 😞 \",text)\n",
    "    text = re.sub(r\"\\(；一_一\\)\",\" 😞 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(=\\_=\\)\",\" 😩 \",text)\n",
    "    text = re.sub(r\"\\(=^\\·^=\\)\",\" 😺 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(=^··^=\\)\",\" 😺 \",text)\n",
    "    text = re.sub(r\"=_^= \",\" 😺 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(\\.\\.\\)\",\" 😔 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(\\._\\.\\)\",\" 😔  \",text)\n",
    "    text = re.sub(r\"\\(・・\\?\",\" 😕 \",text)\n",
    "    text = re.sub(r\"\\(\\?_\\?\\)\",\" 😕 \",text)\n",
    "\n",
    "    text = re.sub(r\">^_^<\",\" 😃 \",text)\n",
    "    text = re.sub(r\"<^\\!^>\",\" 😃 \",text)\n",
    "    text = re.sub(r\"^/^\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(\\*^_^\\*\\)\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(^^\\)\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(^\\.^\\)\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(^_^\\.\\)\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(^_^\\)\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(^J^\\)\",\"  😃  \",text)\n",
    "    text = re.sub(r\"\\(\\*^\\.^\\*\\)\",\" 😃  \",text)\n",
    "    text = re.sub(r\"\\(^—^\\）\",\" 😃 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(#^.^#\\)\",\" 😃 \",text)\n",
    "    text = re.sub(r\"\\(^—^\\)\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(;_;\\)/~~~\",\"  👋  \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(^.^\\)/~~~\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)/~~~ \\($··\\)/~~~\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(T_T\\)/~~~\",\" 👋 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(ToT\\)/~~~\",\" 👋 \",text)\n",
    "    text = re.sub(r\"\\(\\*^0^\\*\\)\",\" 😍 \",text)\n",
    "    text = re.sub(r\"\\(\\*_\\*\\)\",\" 😍 \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(\\*_\\*;\",\" 😍 \",text)\n",
    "    text = re.sub(r\"\\(+_+\\) \\(@_@\\)\",\" 😍 \",text)\n",
    "    text = re.sub(r\"o\\.O\",\" 😮 \",text)\n",
    "    text = re.sub(r\"\\(o\\.o\\)\",\" 😮 \",text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessString(text):\n",
    "    repeatedChars = ['.', '?', '!', ',']\n",
    "    for c in repeatedChars:\n",
    "        lineSplit = text.split(c)\n",
    "        while True:\n",
    "            try:\n",
    "                lineSplit.remove('')\n",
    "            except:\n",
    "                break\n",
    "        cSpace = ' ' + c + ' '\n",
    "        text = cSpace.join(lineSplit)\n",
    "        \n",
    "    text = fix_apos(text)\n",
    "    \n",
    "    return str(text)\n",
    "    \n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    test_set = []\n",
    "    u1 = []\n",
    "    u2 = []\n",
    "    u3 = []\n",
    "    smil_1 = []\n",
    "    smil_2 = []\n",
    "    smil_3 = []\n",
    "    smileys = []\n",
    "    raw1 = []\n",
    "    raw2 = []\n",
    "    raw3 = []\n",
    "    \n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            \n",
    "            fix_emoji_line = str2emoji(line)\n",
    "            splitted_line = fix_emoji_line.strip().split('\\t')\n",
    "            \n",
    "            raw1.append(splitted_line[1])\n",
    "            raw2.append(splitted_line[2])\n",
    "            raw3.append(splitted_line[3]) \n",
    "            \n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '\n",
    "                line = cSpace.join(lineSplit)\n",
    "                \n",
    "            line = line.strip().split('\\t')\n",
    "                       \n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "                test_set.append([line[4]])\n",
    "\n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            #Replace non-unicode smilys with unicode\n",
    "            conv = str2emoji(conv)\n",
    "            \n",
    "            #Separate smilys w unicode\n",
    "            conv = add_space(conv)\n",
    "            \n",
    "            #Many of the words not in embeddings are problematic due to apostrophes e.g. didn't\n",
    "            conv = fix_apos(conv)\n",
    "\n",
    "            #Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            #Find all smilys as array of strings\n",
    "            row_smileys = remove_text(conv)\n",
    "\n",
    "            #Do the same operations for each turn\n",
    "            u1_line = conv.split(' <eos> ')[0]\n",
    "            u2_line = conv.split(' <eos> ')[1]\n",
    "            u3_line = conv.split(' <eos> ')[2]\n",
    "            \n",
    "            #separate smilys per turn\n",
    "            s1 = remove_text(u1_line)\n",
    "            s2 = remove_text(u2_line)\n",
    "            s3 = remove_text(u3_line)\n",
    "            \n",
    "            #remove double spaaces\n",
    "            smil_1.append(re.sub(duplicateSpacePattern, ' ', s1))\n",
    "            smil_2.append(re.sub(duplicateSpacePattern, ' ', s2))\n",
    "            smil_3.append(re.sub(duplicateSpacePattern, ' ', s3))\n",
    "                        \n",
    "            u1.append(re.sub(duplicateSpacePattern, ' ', u1_line.lower()))\n",
    "            u2.append(re.sub(duplicateSpacePattern, ' ', u2_line.lower()))\n",
    "            u3.append(re.sub(duplicateSpacePattern, ' ', u3_line.lower()))\n",
    "            smileys.append(row_smileys)            \n",
    "\n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels, u1, u2, u3, smileys, smil_1, smil_2, smil_3, raw1, raw2, raw3\n",
    "    else:\n",
    "        return indices, conversations, u1, u2, u3, smileys\n",
    "    \n",
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    with io.open(os.path.join('glove.6B', 'glove.6B.50d.txt'), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "\n",
    "    # Minimum word index of any word is 1.\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, 50))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "\n",
    "    return embeddingMatrix\n",
    "\n",
    "def find_converage(big_dict, small_dict):\n",
    "    not_in_small = {}\n",
    "    missing = 0\n",
    "    exist_both = 0\n",
    "    for key in small_dict:\n",
    "        if key in big_dict:\n",
    "            exist_both += 1\n",
    "        else:\n",
    "            not_in_small[key] = 0\n",
    "            missing += 1\n",
    "    coverage = exist_both/(exist_both + missing)\n",
    "    return [coverage, missing, exist_both], not_in_small\n",
    "\n",
    "def add_word(string, word):\n",
    "    return 1 if word in string.split(' ') else 0\n",
    "\n",
    "def find_number_missing(not_in_small, tokenizer_input_data):\n",
    "    loop_count = 0\n",
    "    for line in tokenizer_input_data:\n",
    "        for key, _ in not_in_small.items():\n",
    "            not_in_small[key] += add_word(line, key)\n",
    "        loop_count += 1\n",
    "        if not loop_count%10000:\n",
    "            print(\"%s of %s\"%(loop_count, len(tokenizer_input_data)))\n",
    "    return not_in_small\n",
    "\n",
    "def fix_apos(word):\n",
    "    replaced_word = re.sub(r\"(it['|´|’]s)\", \"it is\", word)\n",
    "    replaced_word = re.sub(r\"(can['|´|’]t)\", \"can not\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(ain['|´|’]t)\", \"am not\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|a-z]{0,10})['|´|’]re\", r\"\\1 are\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|a-z]{0,10})['|´|’]ll\", r\"\\1 will\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|a-z]{0,10})['|´|’]ve\", r\"\\1 have\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(he|who|how|when|there)['|´|’]s\", r\"\\1 is\", replaced_word)\n",
    "    replaced_word = re.sub(r\"that['|´|’]s\", r\"that is\", replaced_word)    \n",
    "    replaced_word = re.sub(r\"what['|´|’]s\", r\"what is\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(let['|´|’]s)\", \"let us\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|i]['|´|’]m)\", \"i am\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(won['|´|’]t)\", \"will not\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(n['|´|’]t)\", \" not\", replaced_word)    \n",
    "    replaced_word = re.sub(r\"['|´|’]\", r\" \", replaced_word)\n",
    "    return replaced_word\n",
    "\n",
    "#separate words, then characters and pad as matrix eg:[[0,0,0,1,2,3],[1,2,3,4,5,6]] \n",
    "def padded_char_vectors(u1, u2, u3):\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=False)\n",
    "    tk.fit_on_texts(u1+u2+u3)\n",
    "    alphabet = \" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0gf123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    t1 = {}\n",
    "    t2 = {}\n",
    "    t3 = {}\n",
    "    word_list_t1 = []\n",
    "    word_list_t2 = []\n",
    "    word_list_t3 = []\n",
    "    padded_seq_t1 = []\n",
    "    padded_seq_t2 = []\n",
    "    padded_seq_t3 = []\n",
    "    \n",
    "    #keras stuff for fixing dict\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "    tk.word_index = char_dict.copy()\n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "    \n",
    "    #crazy stupid looping to get everything on the right shape\n",
    "    #for text_to_sequences you want a list of lists of each letter per word per sentence\n",
    "    for k, sentence in enumerate(u1):\n",
    "        t1[k] = sentence.split(' ')\n",
    "        t1[k] = [list(word) for word in t1[k]]\n",
    "    for k, sentence in enumerate(u2):\n",
    "        t2[k] = sentence.split(' ')\n",
    "        t2[k] = [list(word) for word in t2[k]]\n",
    "    for k, sentence in enumerate(u3):\n",
    "        t3[k] = sentence.split(' ')\n",
    "        t3[k] = [list(word) for word in t3[k]]\n",
    "        \n",
    "    #remaking to list of list        \n",
    "    for key in t1:\n",
    "        word_list_t1.append(t1[key])\n",
    "    for key in t2:\n",
    "        word_list_t2.append(t2[key])\n",
    "    for key in t3:\n",
    "        word_list_t3.append(t3[key])\n",
    "    \n",
    "    #pad the right things\n",
    "    for word_per_turn in word_list_t1:\n",
    "        seq_t1 = tk.texts_to_sequences(word_per_turn)\n",
    "        padded1 = pad_sequences(seq_t1, maxlen=CHAR_MAX_LEN)\n",
    "        padded_seq_t1.append(pad_array(padded1, CHAR_MAX_LEN))\n",
    "        \n",
    "    for word_per_turn in word_list_t2:\n",
    "        seq_t2 = tk.texts_to_sequences(word_per_turn)\n",
    "        padded2 = pad_sequences(seq_t2, maxlen=CHAR_MAX_LEN)\n",
    "        padded_seq_t2.append(pad_array(padded2, CHAR_MAX_LEN))\n",
    "    \n",
    "    for word_per_turn in word_list_t3:\n",
    "        seq_t3 = tk.texts_to_sequences(word_per_turn)\n",
    "        padded3 = pad_sequences(seq_t3, maxlen=CHAR_MAX_LEN)\n",
    "        padded_seq_t3.append(pad_array(padded3, CHAR_MAX_LEN))        \n",
    "    \n",
    "    return padded_seq_t1, padded_seq_t2, padded_seq_t3\n",
    "\n",
    "#speparate characters and pad sentences, compared to above that uses matrices in which each row contains \n",
    "#character representation for each word in a sentence\n",
    "def pad_sentence(u1, u2, u3):\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=False)\n",
    "    tk.fit_on_texts(u1+u2+u3)\n",
    "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0gf123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    t1 = {}\n",
    "    t2 = {}\n",
    "    t3 = {}\n",
    "    char_list_t1 = []\n",
    "    char_list_t2 = []\n",
    "    char_list_t3 = []\n",
    "    padded_seq_t1 = []\n",
    "    padded_seq_t2 = []\n",
    "    padded_seq_t3 = []\n",
    "    \n",
    "    #keras stuff for fixing dict\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "    tk.word_index = char_dict.copy()\n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "    \n",
    "    for sentence in u1:\n",
    "        char_list_t1.append(list(sentence))\n",
    "    char_list_t1 = tk.texts_to_sequences(char_list_t1)\n",
    "    char_list_t1 = pad_sequences(char_list_t1, maxlen=CHAR_MAX_LEN)\n",
    "    \n",
    "    for sentence in u2:\n",
    "        char_list_t2.append(list(sentence))\n",
    "    char_list_t2 = tk.texts_to_sequences(char_list_t2)\n",
    "    char_list_t2 = pad_sequences(char_list_t2, maxlen=CHAR_MAX_LEN)\n",
    "    \n",
    "    for sentence in u3:\n",
    "        char_list_t3.append(list(sentence))\n",
    "    char_list_t3 = tk.texts_to_sequences(char_list_t3)\n",
    "    char_list_t3 = pad_sequences(char_list_t3, maxlen=CHAR_MAX_LEN)\n",
    "    \n",
    "    padded_seq_t1 = np.asarray(char_list_t1)\n",
    "    padded_seq_t2 = np.asarray(char_list_t2)\n",
    "    padded_seq_t3 = np.asarray(char_list_t3)\n",
    "    \n",
    "    return padded_seq_t1, padded_seq_t2, padded_seq_t3\n",
    "\n",
    "\n",
    "def pad_array(matrix, pad_len):\n",
    "    pad_diff = pad_len - matrix.shape[0]\n",
    "    if pad_diff < 0:\n",
    "        pad_diff = pad_len\n",
    "    max_pad = np.zeros((pad_diff, matrix.shape[1]))\n",
    "    padded_matrix = np.concatenate((matrix, max_pad), axis=0)\n",
    "    padded_matrix = padded_matrix[0:pad_len,:]\n",
    "    return padded_matrix\n",
    "        \n",
    "    \n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, conversations, labels, v1, v2, v3, smileys, smil_v1, smil_v2, smil_v3, r1, r2, r3 = preprocessData('natemusMasters/starterkitdata/dev.txt', 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_MAX_LEN = 32\n",
    "t1, t2, t3 = padded_char_vectors(r1, r2, r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3 = pad_sentence(r1, r2, r3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CHAR_MAX_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bd749051d8d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-bd749051d8d8>\u001b[0m in \u001b[0;36mbuildModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mchar_input1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAR_MAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mchar_input2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAR_MAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchar_input3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAR_MAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CHAR_MAX_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "def buildModel():\n",
    "    char_input1 = Input(shape=(CHAR_MAX_LEN,))\n",
    "    char_input2 = Input(shape=(CHAR_MAX_LEN,))\n",
    "    char_input3 = Input(shape=(CHAR_MAX_LEN,))\n",
    "\n",
    "    conv = Conv1D(kernel_size=4, filters=30, padding='same', activation='tanh', strides=1)\n",
    "    lstm = Bidirectional(LSTM(units=256,return_sequences=False, recurrent_dropout=0.2))\n",
    "    \n",
    "    emb1 = Embedding(97, 32)(char_input1)\n",
    "    emb2 = Embedding(97, 32)(char_input2)\n",
    "    emb3 = Embedding(97, 32)(char_input3)\n",
    "    \n",
    "    conv1 = conv(emb1)\n",
    "    conv2 = conv(emb2)\n",
    "    conv3 = conv(emb3)\n",
    "    \n",
    "    conc_conv = Concatenate(axis=-1)([conv1, conv2, conv3])\n",
    "    lstm_char = lstm(conc_conv)\n",
    "    \n",
    "    flat = Flatten()(conc_conv)\n",
    "    \n",
    "    model_output = Dense(4, activation='sigmoid')(flat)\n",
    "    \n",
    "    model = Model([char_input1, char_input2, char_input3], model_output)\n",
    "    adam = optimizers.adam(lr=0.1)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = buildModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stri = \" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0gf123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "len(stri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = to_categorical(np.asarray(labels))\n",
    "m1 = model.fit([s1,s2,s3], labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('predictions.csv')\n",
    "df2 = pd.read_csv('data_w_predictions.csv')\n",
    "\n",
    "df = pd.DataFrame({'conv': df1.Conversation, 'pred_model': df2.predictions, 'pred_BERT': df1.Predicted, 'true_label': df2.label})\n",
    "df.pred_model = df.pred_model.apply(lambda x: emotion2label[x])\n",
    "df.true_label = df.true_label.apply(lambda x: emotion2label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = 200\n",
    "number_diff = df.loc[df['pred_model'] != df['pred_BERT']].shape[0]\n",
    "number_eq = df.loc[df['pred_model'] == df['pred_BERT']].shape[0]\n",
    "bert_correct = df.loc[df['true_label'] == df['pred_BERT']].shape[0]\n",
    "model_correct = df.loc[df['true_label'] == df['pred_model']].shape[0]\n",
    "bert_not_model = df.loc[(df['true_label'] == df['pred_BERT']) & (df['true_label'] != df['pred_model'])].shape[0]\n",
    "model_not_bert = df.loc[(df['true_label'] != df['pred_BERT']) & (df['true_label'] == df['pred_model'])].shape[0]\n",
    "stats = {'number_diff': number_diff, 'number_equal': number_eq, 'bert_correct': bert_correct, 'model_correct': model_correct, 'bert_right_not_model': bert_not_model, 'model_right_not_bert': model_not_bert}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_not_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/train.txt', sep='\\t')\n",
    "dev = pd.read_csv('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/dev.txt', sep='\\t')\n",
    "test = pd.read_csv('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/testwithlabels.txt', sep='\\t')\n",
    "all_sets = [train, dev, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7ecafbf11806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_emoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_or_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_or_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_or_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def other_or_emo(emotion):\n",
    "    emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "    return 0 if emotion2label[emotion]==0 else 1\n",
    "\n",
    "def remove_emoji(text):\n",
    "    return ''.join([char for char in list(text) if not is_emoji(char)]).strip()\n",
    "\n",
    "train.label = train.label.apply(other_or_emo)\n",
    "dev.label = dev.label.apply(other_or_emo)\n",
    "test.label = test.label.apply(other_or_emo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData2(dir):\n",
    "    df = pd.read_csv(dir, sep='\\t')\n",
    "\n",
    "    df.label = df.label.apply(other_or_emo)\n",
    "    labels = df.label.tolist()\n",
    "\n",
    "    df['t1_len'] = df['turn1'].apply(count_length)\n",
    "    df['t2_len'] = df['turn2'].apply(count_length)\n",
    "    df['t3_len'] = df['turn3'].apply(count_length)\n",
    "\n",
    "    df['t1_upper'] = df['turn1'].apply(count_upper_case)\n",
    "    df['t2_upper'] = df['turn2'].apply(count_upper_case)\n",
    "    df['t3_upper'] = df['turn3'].apply(count_upper_case)\n",
    "    \n",
    "    df['t1_smil'] = df['turn1'].apply(str2emoji).apply(lambda x: sum([1 for s in x if is_emoji(s)]))\n",
    "    df['t2_smil'] = df['turn2'].apply(str2emoji).apply(lambda x: sum([1 for s in x if is_emoji(s)]))\n",
    "    df['t3_smil'] = df['turn3'].apply(str2emoji).apply(lambda x: sum([1 for s in x if is_emoji(s)]))\n",
    "    \n",
    "    df['turn1'] = df['turn1'].apply(str2emoji).apply(remove_emoji).apply(lambda x: x.lower()).apply(preprocessString)\n",
    "    df['turn2'] = df['turn2'].apply(str2emoji).apply(remove_emoji).apply(lambda x: x.lower()).apply(preprocessString)\n",
    "    df['turn3'] = df['turn3'].apply(str2emoji).apply(remove_emoji).apply(lambda x: x.lower()).apply(preprocessString)\n",
    "\n",
    "    ind = df.id.tolist()\n",
    "\n",
    "    t1, t2, t3 = np.asarray(df.turn1), np.asarray(df.turn2), np.asarray(df.turn3)\n",
    "    len1, len2, len3 = np.asarray(df.t1_len), np.asarray(df.t2_len), np.asarray(df.t3_len)\n",
    "    smil1, smil2, smil3 = np.asarray(df.t1_smil), np.asarray(df.t2_smil), np.asarray(df.t3_smil)\n",
    "    case1, case2, case3 = np.asarray(df.t1_upper), np.asarray(df.t2_upper), np.asarray(df.t3_upper)\n",
    "    \n",
    "    return ind, labels, t1, t2, t3, len1, len2, len3, case1, case2, case3, smil1, smil2, smil3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens...\n",
      "Pad everything\n"
     ]
    }
   ],
   "source": [
    "validationDataPath = '/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/dev.txt'\n",
    "testDataPath = '/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/testwithlabels.txt'\n",
    "trainDataPath = '/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/train.txt'\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "ind, labels, t1, t2, t3, len1, len2, len3, case1, case2, case3, smil1, smil2, smil3 = preprocessData2(trainDataPath)\n",
    "ind_val, validationLabels, t1_val, t2_val, t3_val, len1_val, len2_val, len3_val, case1_val, case2_val, case3_val, smil1_val, smil2_val, smil3_val = preprocessData2(validationDataPath)\n",
    "ind_test, testLabels, t1_test, t2_test, t3_test, len1_test, len2_test, len3_test, case1_test, case2_test, case3_test, smil1_test, smil2_test, smil3_test = preprocessData2(testDataPath)\n",
    "\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(t1+t2+t3)\n",
    "wordIndex = tokenizer.word_index\n",
    "\n",
    "t1, t2, t3 = tokenizer.texts_to_sequences(t1), tokenizer.texts_to_sequences(t2), tokenizer.texts_to_sequences(t3)\n",
    "t1_val, t2_val, t3_val = tokenizer.texts_to_sequences(t1_val), tokenizer.texts_to_sequences(t2_val), tokenizer.texts_to_sequences(t3_val)\n",
    "t1_test, t2_test, t3_test = tokenizer.texts_to_sequences(t1_test), tokenizer.texts_to_sequences(t2_test), tokenizer.texts_to_sequences(t3_test)\n",
    "\n",
    "print(\"Pad everything\")\n",
    "t1, t2, t3 = pad_sequences(t1, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "t1_val, t2_val, t3_val = pad_sequences(t1_val, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2_val, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "t1_test, t2_test, t3_test = pad_sequences(t1_test, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2_test, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "validationLabels = to_categorical(np.asarray(validationLabels))\n",
    "testLabels = to_categorical(np.asarray(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(ind)\n",
    "t1 = t1[ind]\n",
    "t2 = t2[ind]\n",
    "t3 = t3[ind]\n",
    "case1 = np.asarray([case1[i] for i in ind])\n",
    "case2 = np.asarray([case2[i] for i in ind])\n",
    "case3 = np.asarray([case3[i] for i in ind])\n",
    "len1 = np.asarray([len1[i] for i in ind])\n",
    "len2 = np.asarray([len2[i] for i in ind])\n",
    "len3 = np.asarray([len3[i] for i in ind])\n",
    "smil1 = np.asarray([smil1[i] for i in ind])\n",
    "smil2 = np.asarray([smil2[i] for i in ind])\n",
    "smil3 = np.asarray([smil3[i] for i in ind])\n",
    "labels = labels[ind]\n",
    "metrics = {\"accuracy\" : [], \"microPrecision\" : [], \"microRecall\" : [], \"microF1\" : []}\n",
    "embMat = np.zeros((20000, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(embeddingMatrix):\n",
    "    \"\"\"Constructs the architecture of the model\n",
    "    Input:\n",
    "        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\n",
    "    Output:\n",
    "        model : three layer lstm model with one layer of meta data\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = Input(shape=(100,), dtype='int32', name='turn_1')\n",
    "    t2 = Input(shape=(100,), dtype='int32', name='turn_2')\n",
    "    t3 = Input(shape=(100,), dtype='int32', name='turn_3')\n",
    "\n",
    "    meta_data = Input(shape=(9,), dtype='float32', name='meta_input')\n",
    "    \n",
    "    ########## mata data layer #############\n",
    "    hidden_layer = Dense(32, activation='relu')\n",
    "    meta_layer = hidden_layer(meta_data)\n",
    "    ########################################\n",
    "\n",
    "    ########## Conversation layer ##########\n",
    "    twitterEmbeddings = Embedding(embeddingMatrix.shape[0], EMBEDDING_DIM, weights=[embeddingMatrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "\n",
    "    emb1 = twitterEmbeddings(t1)\n",
    "    emb2 = twitterEmbeddings(t2)\n",
    "    emb3 = twitterEmbeddings(t3)\n",
    "\n",
    "    #LSTM layers, need to define a new one for different embeddings\n",
    "    lstm = Bidirectional(LSTM(32, dropout=DROPOUT, return_sequences=True))\n",
    "    second_layer = LSTM(64, dropout=DROPOUT)\n",
    "\n",
    "    lstm1 = lstm(emb1)\n",
    "    lstm2 = lstm(emb2)\n",
    "    lstm3 = lstm(emb3)\n",
    "\n",
    "    concatenated_lstm = Concatenate(axis=-1)([lstm1, lstm2, lstm3])\n",
    "    concatenated_lstm = Dropout(DROPOUT)(concatenated_lstm)\n",
    "    concatenated_lstm = second_layer(concatenated_lstm)\n",
    "    concatenated_lstm = Dropout(DROPOUT)(concatenated_lstm)\n",
    "    \n",
    "    ############ Merging ############\n",
    "    merge_out = Concatenate(axis=-1)([concatenated_lstm, meta_layer])\n",
    "\n",
    "    #output\n",
    "    model_output = Dense(2, activation='sigmoid')(merge_out)\n",
    "    model_output = Dense()\n",
    "    model = Model([t1, t2, t3, meta_data], model_output)\n",
    "\n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    adam =  optimizers.adam(lr=LEARNING_RATE)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up mo.. mod... mod..EL.. MODEL MOOODEEEL\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "turn_1 (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "turn_2 (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "turn_3 (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 200)     4000000     turn_1[0][0]                     \n",
      "                                                                 turn_2[0][0]                     \n",
      "                                                                 turn_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100, 64)      59648       embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "                                                                 embedding_4[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 100, 192)     0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "                                                                 bidirectional_4[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 100, 192)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 64)           65792       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "meta_input (InputLayer)         (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           320         meta_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           dropout_8[0][0]                  \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            194         concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,125,954\n",
      "Trainable params: 4,125,954\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2322\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2324\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'bidirectional_4_2/while_1/TensorArrayReadV3' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2326\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'bidirectional_4_2/while_1/TensorArrayReadV3' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6d63a87cb439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membMat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \"\"\"\n\u001b[0;32m-> 2757\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 630\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 814\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    815\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 814\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    815\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\u001b[0m in \u001b[0;36m_TensorArrayReadGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    103\u001b[0m   g = (tensor_array_ops.TensorArray(dtype=dtype, handle=handle, flow=flow,\n\u001b[1;32m    104\u001b[0m                                     colocate_with_first_write_call=False)\n\u001b[0;32m--> 105\u001b[0;31m        .grad(source=grad_source, flow=flow))\n\u001b[0m\u001b[1;32m    106\u001b[0m   \u001b[0mw_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, source, flow, name)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, source, flow, name)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorArrayGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         g_handle, unused_flow = gen_data_flow_ops.tensor_array_grad_v3(\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6023\u001b[0m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6024\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6025\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6026\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6027\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5225\u001b[0m         default.building_function, default.as_default)\n\u001b[1;32m   5226\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5227\u001b[0;31m       with super(_DefaultGraphStack, self).get_controller(\n\u001b[0m\u001b[1;32m   5228\u001b[0m           default) as g, context.graph_mode():\n\u001b[1;32m   5229\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Set up mo.. mod... mod..EL.. MODEL MOOODEEEL\")\n",
    "\n",
    "NUM_EPOCHS=30\n",
    "LEARNING_RATE=0.005\n",
    "LSTM_DIM = 128\n",
    "BATCH_SIZE = 200\n",
    "EMBEDDING_DIM = 200\n",
    "DROPOUT = 0.3\n",
    "\n",
    "meta_data = np.asarray([len1, len2, len3, case1, case2, case3, smil1, smil2, smil3]).T\n",
    "meta_data_val = np.asarray([len1_val, len2_val, len3_val, case1_val, case2_val, case3_val, smil1_val, smil2_val, smil3_val]).T\n",
    "meta_data_test = np.asarray([len1_test, len2_test, len3_test, case1_test, case2_test, case3_test, smil1_test, smil2_test, smil3_test]).T\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "mc = ModelCheckpoint('EP%d_LR%de-5_LDim%d_BS%d.h5'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "model = buildModel(embMat)\n",
    "model.summary()\n",
    "history = model.fit([t1, t2, t3, meta_data], labels, validation_data=([t1_val, t2_val, t3_val, meta_data_val], validationLabels), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=2, callbacks=[es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

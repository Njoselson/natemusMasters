{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0429 12:25:55.234136 140736563217344 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "#Please use python 3.5 or above\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, Concatenate, Bidirectional, Conv2D, TimeDistributed, Add, Conv1D, Reshape, Flatten, Dropout, MaxPooling1D\n",
    "from keras import optimizers\n",
    "from keras.models import load_model, Model\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return s in emoji.UNICODE_EMOJI\n",
    "\n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char + ' ' if is_emoji(char) else char for char in text).strip()\n",
    "\n",
    "def remove_text(text):\n",
    "    words = text.split(' ')\n",
    "    emojis = ' '.join([word for word in words if is_emoji(word)])\n",
    "    return emojis\n",
    "\n",
    "def count_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_upper_case(text):\n",
    "    return sum(1 for c in text if c.isupper())\n",
    "\n",
    "def str2emoji(text):\n",
    "    text = re.sub(r\":â€‘\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\)\\)\\)\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\)\\)\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\)\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"\\(:\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":-\\]\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\]\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":-3\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":3\",\"  â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":->\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"</3\",' ğŸ’” ',text)\n",
    "    text = re.sub(r\"<3\",\" â¤ï¸ \",text)\n",
    "    text = re.sub(r\":>\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"8-\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":o\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":-\\}\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\}\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":-\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":c\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\^\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"=\\]\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"=\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":â€‘D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"8â€‘D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"8D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"Xâ€‘D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"xD\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"xd\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"XD\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"=D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"=3\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"B\\^D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":-\\)\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":â€‘\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":-\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":-d\", \" ğŸ¤¤ \",text)\n",
    "    text = re.sub(r\":d\", \" ğŸ¤¤ \",text)\n",
    "    text = re.sub(r\"=d\", \" ğŸ¤¤ \",text)\n",
    "\n",
    "    text = re.sub(r\":\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":â€‘c\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":c\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":â€‘<\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":<\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":â€‘\\[\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":\\[\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":-\\|\\|\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\">:\\[\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":\\{\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":@\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\">:\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":'â€‘\\(\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\":'\\(\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\":'â€‘\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":'\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"Dâ€‘':\",\" ğŸ˜§ \",text)\n",
    "    text = re.sub(r\"D:<\",\" ğŸ˜¨ \",text)\n",
    "    text = re.sub(r\"D:\",\" ğŸ˜§ \",text)\n",
    "    text = re.sub(r\"D8\",\" ğŸ˜§ \",text)\n",
    "    text = re.sub(r\"D;\",\" ğŸ˜§ \",text)\n",
    "    text = re.sub(r\"D=\",\" ğŸ˜§ \",text)\n",
    "    text = re.sub(r\"DX\",\" ğŸ˜§ \",text)\n",
    "    text = re.sub(r\":â€‘O\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\":O\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\":â€‘o\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\":o\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\":-0\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\"8â€‘0\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\">:O\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\":-\\*\",\" ğŸ˜— \",text)\n",
    "    text = re.sub(r\":\\*\",\" ğŸ˜— \",text)\n",
    "    text = re.sub(r\":X\",\" ğŸ˜— \",text)\n",
    "    text = re.sub(r\";â€‘\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\"\\*-\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\"\\*\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";â€‘\\]\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";\\]\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";\\^\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\":â€‘,\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";D\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\":â€‘P\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":â€‘p\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":P\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":p\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\"Xâ€‘P\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":â€‘Ã\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":Ã\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":b\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\"d:\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\"=p\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\">:P\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\":â€‘/\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\":/\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\":-\\[\\.\\]\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\">:/\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"=/\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\":L\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"=L\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\":S\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\":â€‘\\|\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\":\\|\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\":$\",\" ğŸ˜³ \",text)\n",
    "    text = re.sub(r\":â€‘x\",\" ğŸ¤ \",text)\n",
    "    text = re.sub(r\":x\",\" ğŸ¤ \",text)\n",
    "    text = re.sub(r\":â€‘#\",\" ğŸ¤ \",text)\n",
    "    text = re.sub(r\":#\",\" ğŸ¤ \",text)\n",
    "    text = re.sub(r\":â€‘&\",\" ğŸ¤ \",text)\n",
    "    text = re.sub(r\":&\",\" ğŸ¤ \",text)\n",
    "    text = re.sub(r\"O:â€‘\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"O:\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0:â€‘3\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0:3\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0:â€‘\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0:\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\":â€‘b\",\" ğŸ˜› \",text)\n",
    "    text = re.sub(r\"0;\\^\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\">:â€‘\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\">:\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"\\}:â€‘\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"\\}:\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"3:â€‘\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"3:\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\">;\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"\\|;â€‘\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\|â€‘O\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\":â€‘J\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"%â€‘\\)\",\" ğŸ˜µ \",text)\n",
    "    text = re.sub(r\"%\\)\",\" ğŸ˜µ \",text)\n",
    "    text = re.sub(r\":-###..\",\" ğŸ¤’ \",text)\n",
    "    text = re.sub(r\":###..\",\" ğŸ¤’ \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)\",\" ğŸ˜£ \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)>\",\" ğŸ˜£ \",text)\n",
    "    text = re.sub(r\"\\(';'\\)\",\" ğŸ‘¶ \",text)\n",
    "    text = re.sub(r\"\\(\\^\\^>``\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^;\\)\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(-_-;\\)\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(~_~;\\) \\(ãƒ»\\.ãƒ»;\\)\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)zzz\",\" ğŸ˜´ \",text)\n",
    "    text = re.sub(r\"\\(\\^_-\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\"\\(\\(\\+_\\+\\)\\)\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"\\(\\+o\\+\\)\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"\\^_\\^\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\)/\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^O\\^\\)ï¼\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^o\\^\\)ï¼\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(__\\)\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"_\\(\\._\\.\\)_\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"<\\(_ _\\)>\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"<m\\(__\\)m>\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"m\\(__\\)m\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"m\\(_ _\\)m\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"\\('_'\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(/_;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(T_T\\) \\(;_;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(;_;\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(;_:\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(;O;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(:_;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(ToT\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\";_;\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\";-;\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\";n;\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\";;\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"Q\\.Q\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"T\\.T\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"QQ\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"Q_Q\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(-\\.-\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"-_-\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(ä¸€ä¸€\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(ï¼›ä¸€_ä¸€\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(=_=\\)\",\" ğŸ˜© \",text)\n",
    "    text = re.sub(r\"\\(=\\^\\Â·\\^=\\)\",\" ğŸ˜º \",text)\n",
    "    text = re.sub(r\"\\(=\\^\\Â·\\Â·\\^=\\)\",\" ğŸ˜º \",text)\n",
    "    text = re.sub(r\"=_\\^= \",\" ğŸ˜º \",text)\n",
    "    text = re.sub(r\"\\(\\.\\.\\)\",\" ğŸ˜” \",text)\n",
    "    text = re.sub(r\"\\(\\._\\.\\)\",\" ğŸ˜” \",text)\n",
    "    text = re.sub(r\"\\(\\ãƒ»\\ãƒ»\\?\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"\\(\\?_\\?\\)\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\">\\^_\\^<\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"<\\^!\\^>\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\^/\\^\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\ï¼ˆ\\*\\^_\\^\\*ï¼‰\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(^\\^\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^\\.\\^\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\.\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^\\^\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^J\\^\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\*\\^\\.\\^\\*\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(\\^â€”\\^\\ï¼‰\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(#\\^\\.\\^#\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\ï¼ˆ\\^â€”\\^\\ï¼‰\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(;_;\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(\\^\\.\\^\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(T_T\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(\\*\\^0\\^\\*\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(\\*_\\*\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(\\*_\\*;\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(\\+_\\+\\) \\(@_@\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(\\*\\^\\^\\)v\",\" ğŸ˜‚ \",text)\n",
    "    text = re.sub(r\"\\(\\^_\\^\\)v\",\" ğŸ˜‚ \",text)\n",
    "    text = re.sub(r\"\\(ãƒ¼ãƒ¼;\\)\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(\\^0_0\\^\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(\\ï¼¾ï½–\\ï¼¾\\)\",\" ğŸ˜€ \",text)\n",
    "    text = re.sub(r\"\\(\\ï¼¾ï½•\\ï¼¾\\)\",\" ğŸ˜€ \",text)\n",
    "    text = re.sub(r\"\\(\\^\\)o\\(\\^\\)\",\" ğŸ˜€ \",text)\n",
    "    text = re.sub(r\"\\(\\^O\\^\\)\",\" ğŸ˜€ \",text)\n",
    "    text = re.sub(r\"\\(\\^o\\^\\)\",\" ğŸ˜€ \",text)\n",
    "    text = re.sub(r\"\\)\\^o\\^\\(\",\" ğŸ˜€ \",text)\n",
    "    text = re.sub(r\":O o_O\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\"o_0\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\"o\\.O\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\"\\(o\\.o\\)\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\"oO\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r':\\â€‘\\)','ğŸ˜ƒ',text)\n",
    "    text = re.sub(r\":\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":-]\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":]\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"8\\-\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":o\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":-}\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":}\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":\\-\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":c\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\":^\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"=]\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"=\\)\",\" â˜ºï¸ \",text)\n",
    "    text = re.sub(r\"B^D\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":-\\)\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":-\\(\",\" â˜¹ï¸ \",text)\n",
    "\n",
    "    text = re.sub(r\":â€‘\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":\\â€‘\\[\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":\\[\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":-\\|\\|\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\">\\:\\[\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":{\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\">\\:\\(\",\" â˜¹ï¸ \",text)\n",
    "    text = re.sub(r\":'â€‘\\(\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\":'\\(\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\":'\\â€‘\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":'\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\":-\\*\",\" ğŸ˜— \",text)\n",
    "    text = re.sub(r\":\\*\",\" ğŸ˜— \",text)\n",
    "    text = re.sub(r\";\\â€‘\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\"\\*\\-\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\"\\*\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";â€‘\\]\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";\\]\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\";^\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\">\\:\\[\\(\\)\\]\",\" ğŸ˜• \",text)\n",
    "\n",
    "    text = re.sub(r\":\\[\\(\\)\\]\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"=\\[\\(\\)\\]\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\":â€‘\\|\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\":\\|\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"O:â€‘\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"O:\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0:â€‘\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0:\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\"0;^\\)\",\" ğŸ˜‡ \",text)\n",
    "    text = re.sub(r\">:â€‘\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\">:\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"}:â€‘\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"}:\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"3:â€‘\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"3:\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\">;\\)\",\" ğŸ˜ˆ \",text)\n",
    "    text = re.sub(r\"\\|;â€‘\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\|â€‘O\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"%â€‘\\)\",\" ğŸ˜µ \",text)\n",
    "    text = re.sub(r\"%\\)\",\" ğŸ˜µ \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)\",\" ğŸ˜£ \",text)\n",
    "    text = re.sub(r\"\\(>_<\\)>\",\" ğŸ˜£ \",text)\n",
    "    text = re.sub(r\"\\(';'\\)\",\" Baby \",text)\n",
    "    text = re.sub(r\"\\(^^>``\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(^_^;\\)\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(-_-;\\)\",\" ğŸ˜“ \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(~_~;\\) \\(ãƒ»\\.ãƒ»;\\)\",\" ğŸ˜“ \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)zzz\",\" ğŸ˜´ \",text)\n",
    "    text = re.sub(r\"\\(^_-\\)\",\" ğŸ˜‰ \",text)\n",
    "    text = re.sub(r\"\\(\\(\\+_\\+\\)\\)\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"\\(\\+o\\+\\)\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"^_^\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(^_^\\)/\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(^O^\\)ï¼\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(__\\)\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"_\\(._.\\)_\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"<\\(_ _\\)>\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"<m\\(__\\)m>\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"m\\(__\\)m\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"m\\(_ _\\)m\",\" ğŸ™‡ \",text)\n",
    "    text = re.sub(r\"\\('_'\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(/_;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(T_T\\) (;_;)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(;_;\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(;_:\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(;O;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(:_;\\)\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(ToT\\)\",\"  ğŸ˜­  \",text)\n",
    "    text = re.sub(r\"Q\\.Q\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"T\\.T\",\" ğŸ˜­ \",text)\n",
    "    text = re.sub(r\"\\(-\\.-\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)\",\" ğŸ˜ \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(ä¸€ä¸€\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(ï¼›ä¸€_ä¸€\\)\",\" ğŸ˜ \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(=\\_=\\)\",\" ğŸ˜© \",text)\n",
    "    text = re.sub(r\"\\(=^\\Â·^=\\)\",\" ğŸ˜º \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(=^Â·Â·^=\\)\",\" ğŸ˜º \",text)\n",
    "    text = re.sub(r\"=_^= \",\" ğŸ˜º \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(\\.\\.\\)\",\" ğŸ˜” \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(\\._\\.\\)\",\" ğŸ˜”  \",text)\n",
    "    text = re.sub(r\"\\(ãƒ»ãƒ»\\?\",\" ğŸ˜• \",text)\n",
    "    text = re.sub(r\"\\(\\?_\\?\\)\",\" ğŸ˜• \",text)\n",
    "\n",
    "    text = re.sub(r\">^_^<\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"<^\\!^>\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"^/^\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(\\*^_^\\*\\)\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(^^\\)\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(^\\.^\\)\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(^_^\\.\\)\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(^_^\\)\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(^J^\\)\",\"  ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(\\*^\\.^\\*\\)\",\" ğŸ˜ƒ  \",text)\n",
    "    text = re.sub(r\"\\(^â€”^\\ï¼‰\",\" ğŸ˜ƒ \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(#^.^#\\)\",\" ğŸ˜ƒ \",text)\n",
    "    text = re.sub(r\"\\(^â€”^\\)\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(;_;\\)/~~~\",\"  ğŸ‘‹  \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(^.^\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(-_-\\)/~~~ \\($Â·Â·\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(T_T\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(ToT\\)/~~~\",\" ğŸ‘‹ \",text)\n",
    "    text = re.sub(r\"\\(\\*^0^\\*\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(\\*_\\*\\)\",\" ğŸ˜ \",text)\n",
    "\n",
    "    text = re.sub(r\"\\(\\*_\\*;\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"\\(+_+\\) \\(@_@\\)\",\" ğŸ˜ \",text)\n",
    "    text = re.sub(r\"o\\.O\",\" ğŸ˜® \",text)\n",
    "    text = re.sub(r\"\\(o\\.o\\)\",\" ğŸ˜® \",text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessString(text):\n",
    "    repeatedChars = ['.', '?', '!', ',']\n",
    "    for c in repeatedChars:\n",
    "        lineSplit = text.split(c)\n",
    "        while True:\n",
    "            try:\n",
    "                lineSplit.remove('')\n",
    "            except:\n",
    "                break\n",
    "        cSpace = ' ' + c + ' '\n",
    "        text = cSpace.join(lineSplit)\n",
    "        \n",
    "    text = fix_apos(text)\n",
    "    \n",
    "    return str(text)\n",
    "    \n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    test_set = []\n",
    "    u1 = []\n",
    "    u2 = []\n",
    "    u3 = []\n",
    "    smil_1 = []\n",
    "    smil_2 = []\n",
    "    smil_3 = []\n",
    "    smileys = []\n",
    "    raw1 = []\n",
    "    raw2 = []\n",
    "    raw3 = []\n",
    "    \n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            \n",
    "            fix_emoji_line = str2emoji(line)\n",
    "            splitted_line = fix_emoji_line.strip().split('\\t')\n",
    "            \n",
    "            raw1.append(splitted_line[1])\n",
    "            raw2.append(splitted_line[2])\n",
    "            raw3.append(splitted_line[3]) \n",
    "            \n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '\n",
    "                line = cSpace.join(lineSplit)\n",
    "                \n",
    "            line = line.strip().split('\\t')\n",
    "                       \n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "                test_set.append([line[4]])\n",
    "\n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            #Replace non-unicode smilys with unicode\n",
    "            conv = str2emoji(conv)\n",
    "            \n",
    "            #Separate smilys w unicode\n",
    "            conv = add_space(conv)\n",
    "            \n",
    "            #Many of the words not in embeddings are problematic due to apostrophes e.g. didn't\n",
    "            conv = fix_apos(conv)\n",
    "\n",
    "            #Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            #Find all smilys as array of strings\n",
    "            row_smileys = remove_text(conv)\n",
    "\n",
    "            #Do the same operations for each turn\n",
    "            u1_line = conv.split(' <eos> ')[0]\n",
    "            u2_line = conv.split(' <eos> ')[1]\n",
    "            u3_line = conv.split(' <eos> ')[2]\n",
    "            \n",
    "            #separate smilys per turn\n",
    "            s1 = remove_text(u1_line)\n",
    "            s2 = remove_text(u2_line)\n",
    "            s3 = remove_text(u3_line)\n",
    "            \n",
    "            #remove double spaaces\n",
    "            smil_1.append(re.sub(duplicateSpacePattern, ' ', s1))\n",
    "            smil_2.append(re.sub(duplicateSpacePattern, ' ', s2))\n",
    "            smil_3.append(re.sub(duplicateSpacePattern, ' ', s3))\n",
    "                        \n",
    "            u1.append(re.sub(duplicateSpacePattern, ' ', u1_line.lower()))\n",
    "            u2.append(re.sub(duplicateSpacePattern, ' ', u2_line.lower()))\n",
    "            u3.append(re.sub(duplicateSpacePattern, ' ', u3_line.lower()))\n",
    "            smileys.append(row_smileys)            \n",
    "\n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels, u1, u2, u3, smileys, smil_1, smil_2, smil_3, raw1, raw2, raw3\n",
    "    else:\n",
    "        return indices, conversations, u1, u2, u3, smileys\n",
    "    \n",
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    with io.open(os.path.join('glove.6B', 'glove.6B.50d.txt'), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "\n",
    "    # Minimum word index of any word is 1.\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, 50))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "\n",
    "    return embeddingMatrix\n",
    "\n",
    "def find_converage(big_dict, small_dict):\n",
    "    not_in_small = {}\n",
    "    missing = 0\n",
    "    exist_both = 0\n",
    "    for key in small_dict:\n",
    "        if key in big_dict:\n",
    "            exist_both += 1\n",
    "        else:\n",
    "            not_in_small[key] = 0\n",
    "            missing += 1\n",
    "    coverage = exist_both/(exist_both + missing)\n",
    "    return [coverage, missing, exist_both], not_in_small\n",
    "\n",
    "def add_word(string, word):\n",
    "    return 1 if word in string.split(' ') else 0\n",
    "\n",
    "def find_number_missing(not_in_small, tokenizer_input_data):\n",
    "    loop_count = 0\n",
    "    for line in tokenizer_input_data:\n",
    "        for key, _ in not_in_small.items():\n",
    "            not_in_small[key] += add_word(line, key)\n",
    "        loop_count += 1\n",
    "        if not loop_count%10000:\n",
    "            print(\"%s of %s\"%(loop_count, len(tokenizer_input_data)))\n",
    "    return not_in_small\n",
    "\n",
    "def fix_apos(word):\n",
    "    replaced_word = re.sub(r\"(it['|Â´|â€™]s)\", \"it is\", word)\n",
    "    replaced_word = re.sub(r\"(can['|Â´|â€™]t)\", \"can not\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(ain['|Â´|â€™]t)\", \"am not\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|a-z]{0,10})['|Â´|â€™]re\", r\"\\1 are\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|a-z]{0,10})['|Â´|â€™]ll\", r\"\\1 will\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|a-z]{0,10})['|Â´|â€™]ve\", r\"\\1 have\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(he|who|how|when|there)['|Â´|â€™]s\", r\"\\1 is\", replaced_word)\n",
    "    replaced_word = re.sub(r\"that['|Â´|â€™]s\", r\"that is\", replaced_word)    \n",
    "    replaced_word = re.sub(r\"what['|Â´|â€™]s\", r\"what is\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(let['|Â´|â€™]s)\", \"let us\", replaced_word)\n",
    "    replaced_word = re.sub(r\"([I|i]['|Â´|â€™]m)\", \"i am\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(won['|Â´|â€™]t)\", \"will not\", replaced_word)\n",
    "    replaced_word = re.sub(r\"(n['|Â´|â€™]t)\", \" not\", replaced_word)    \n",
    "    replaced_word = re.sub(r\"['|Â´|â€™]\", r\" \", replaced_word)\n",
    "    return replaced_word\n",
    "\n",
    "#separate words, then characters and pad as matrix eg:[[0,0,0,1,2,3],[1,2,3,4,5,6]] \n",
    "def padded_char_vectors(u1, u2, u3):\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=False)\n",
    "    tk.fit_on_texts(u1+u2+u3)\n",
    "    alphabet = \" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0gf123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    t1 = {}\n",
    "    t2 = {}\n",
    "    t3 = {}\n",
    "    word_list_t1 = []\n",
    "    word_list_t2 = []\n",
    "    word_list_t3 = []\n",
    "    padded_seq_t1 = []\n",
    "    padded_seq_t2 = []\n",
    "    padded_seq_t3 = []\n",
    "    \n",
    "    #keras stuff for fixing dict\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "    tk.word_index = char_dict.copy()\n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "    \n",
    "    #crazy stupid looping to get everything on the right shape\n",
    "    #for text_to_sequences you want a list of lists of each letter per word per sentence\n",
    "    for k, sentence in enumerate(u1):\n",
    "        t1[k] = sentence.split(' ')\n",
    "        t1[k] = [list(word) for word in t1[k]]\n",
    "    for k, sentence in enumerate(u2):\n",
    "        t2[k] = sentence.split(' ')\n",
    "        t2[k] = [list(word) for word in t2[k]]\n",
    "    for k, sentence in enumerate(u3):\n",
    "        t3[k] = sentence.split(' ')\n",
    "        t3[k] = [list(word) for word in t3[k]]\n",
    "        \n",
    "    #remaking to list of list        \n",
    "    for key in t1:\n",
    "        word_list_t1.append(t1[key])\n",
    "    for key in t2:\n",
    "        word_list_t2.append(t2[key])\n",
    "    for key in t3:\n",
    "        word_list_t3.append(t3[key])\n",
    "    \n",
    "    #pad the right things\n",
    "    for word_per_turn in word_list_t1:\n",
    "        seq_t1 = tk.texts_to_sequences(word_per_turn)\n",
    "        padded1 = pad_sequences(seq_t1, maxlen=CHAR_MAX_LEN)\n",
    "        padded_seq_t1.append(pad_array(padded1, CHAR_MAX_LEN))\n",
    "        \n",
    "    for word_per_turn in word_list_t2:\n",
    "        seq_t2 = tk.texts_to_sequences(word_per_turn)\n",
    "        padded2 = pad_sequences(seq_t2, maxlen=CHAR_MAX_LEN)\n",
    "        padded_seq_t2.append(pad_array(padded2, CHAR_MAX_LEN))\n",
    "    \n",
    "    for word_per_turn in word_list_t3:\n",
    "        seq_t3 = tk.texts_to_sequences(word_per_turn)\n",
    "        padded3 = pad_sequences(seq_t3, maxlen=CHAR_MAX_LEN)\n",
    "        padded_seq_t3.append(pad_array(padded3, CHAR_MAX_LEN))        \n",
    "    \n",
    "    return padded_seq_t1, padded_seq_t2, padded_seq_t3\n",
    "\n",
    "#speparate characters and pad sentences, compared to above that uses matrices in which each row contains \n",
    "#character representation for each word in a sentence\n",
    "def pad_sentence(u1, u2, u3):\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=False)\n",
    "    tk.fit_on_texts(u1+u2+u3)\n",
    "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0gf123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    t1 = {}\n",
    "    t2 = {}\n",
    "    t3 = {}\n",
    "    char_list_t1 = []\n",
    "    char_list_t2 = []\n",
    "    char_list_t3 = []\n",
    "    padded_seq_t1 = []\n",
    "    padded_seq_t2 = []\n",
    "    padded_seq_t3 = []\n",
    "    \n",
    "    #keras stuff for fixing dict\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "    tk.word_index = char_dict.copy()\n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "    \n",
    "    for sentence in u1:\n",
    "        char_list_t1.append(list(sentence))\n",
    "    char_list_t1 = tk.texts_to_sequences(char_list_t1)\n",
    "    char_list_t1 = pad_sequences(char_list_t1, maxlen=CHAR_MAX_LEN)\n",
    "    \n",
    "    for sentence in u2:\n",
    "        char_list_t2.append(list(sentence))\n",
    "    char_list_t2 = tk.texts_to_sequences(char_list_t2)\n",
    "    char_list_t2 = pad_sequences(char_list_t2, maxlen=CHAR_MAX_LEN)\n",
    "    \n",
    "    for sentence in u3:\n",
    "        char_list_t3.append(list(sentence))\n",
    "    char_list_t3 = tk.texts_to_sequences(char_list_t3)\n",
    "    char_list_t3 = pad_sequences(char_list_t3, maxlen=CHAR_MAX_LEN)\n",
    "    \n",
    "    padded_seq_t1 = np.asarray(char_list_t1)\n",
    "    padded_seq_t2 = np.asarray(char_list_t2)\n",
    "    padded_seq_t3 = np.asarray(char_list_t3)\n",
    "    \n",
    "    return padded_seq_t1, padded_seq_t2, padded_seq_t3\n",
    "\n",
    "\n",
    "def pad_array(matrix, pad_len):\n",
    "    pad_diff = pad_len - matrix.shape[0]\n",
    "    if pad_diff < 0:\n",
    "        pad_diff = pad_len\n",
    "    max_pad = np.zeros((pad_diff, matrix.shape[1]))\n",
    "    padded_matrix = np.concatenate((matrix, max_pad), axis=0)\n",
    "    padded_matrix = padded_matrix[0:pad_len,:]\n",
    "    return padded_matrix\n",
    "        \n",
    "    \n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, conversations, labels, v1, v2, v3, smileys, smil_v1, smil_v2, smil_v3, r1, r2, r3 = preprocessData('natemusMasters/starterkitdata/dev.txt', 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_MAX_LEN = 32\n",
    "t1, t2, t3 = padded_char_vectors(r1, r2, r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3 = pad_sentence(r1, r2, r3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CHAR_MAX_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bd749051d8d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-bd749051d8d8>\u001b[0m in \u001b[0;36mbuildModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mchar_input1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAR_MAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mchar_input2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAR_MAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchar_input3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHAR_MAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CHAR_MAX_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "def buildModel():\n",
    "    char_input1 = Input(shape=(CHAR_MAX_LEN,))\n",
    "    char_input2 = Input(shape=(CHAR_MAX_LEN,))\n",
    "    char_input3 = Input(shape=(CHAR_MAX_LEN,))\n",
    "\n",
    "    conv = Conv1D(kernel_size=4, filters=30, padding='same', activation='tanh', strides=1)\n",
    "    lstm = Bidirectional(LSTM(units=256,return_sequences=False, recurrent_dropout=0.2))\n",
    "    \n",
    "    emb1 = Embedding(97, 32)(char_input1)\n",
    "    emb2 = Embedding(97, 32)(char_input2)\n",
    "    emb3 = Embedding(97, 32)(char_input3)\n",
    "    \n",
    "    conv1 = conv(emb1)\n",
    "    conv2 = conv(emb2)\n",
    "    conv3 = conv(emb3)\n",
    "    \n",
    "    conc_conv = Concatenate(axis=-1)([conv1, conv2, conv3])\n",
    "    lstm_char = lstm(conc_conv)\n",
    "    \n",
    "    flat = Flatten()(conc_conv)\n",
    "    \n",
    "    model_output = Dense(4, activation='sigmoid')(flat)\n",
    "    \n",
    "    model = Model([char_input1, char_input2, char_input3], model_output)\n",
    "    adam = optimizers.adam(lr=0.1)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = buildModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stri = \" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0gf123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "len(stri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2 = to_categorical(np.asarray(labels))\n",
    "m1 = model.fit([s1,s2,s3], labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('predictions.csv')\n",
    "df2 = pd.read_csv('data_w_predictions.csv')\n",
    "\n",
    "df = pd.DataFrame({'conv': df1.Conversation, 'pred_model': df2.predictions, 'pred_BERT': df1.Predicted, 'true_label': df2.label})\n",
    "df.pred_model = df.pred_model.apply(lambda x: emotion2label[x])\n",
    "df.true_label = df.true_label.apply(lambda x: emotion2label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = 200\n",
    "number_diff = df.loc[df['pred_model'] != df['pred_BERT']].shape[0]\n",
    "number_eq = df.loc[df['pred_model'] == df['pred_BERT']].shape[0]\n",
    "bert_correct = df.loc[df['true_label'] == df['pred_BERT']].shape[0]\n",
    "model_correct = df.loc[df['true_label'] == df['pred_model']].shape[0]\n",
    "bert_not_model = df.loc[(df['true_label'] == df['pred_BERT']) & (df['true_label'] != df['pred_model'])].shape[0]\n",
    "model_not_bert = df.loc[(df['true_label'] != df['pred_BERT']) & (df['true_label'] == df['pred_model'])].shape[0]\n",
    "stats = {'number_diff': number_diff, 'number_equal': number_eq, 'bert_correct': bert_correct, 'model_correct': model_correct, 'bert_right_not_model': bert_not_model, 'model_right_not_bert': model_not_bert}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_not_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/train.txt', sep='\\t')\n",
    "dev = pd.read_csv('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/dev.txt', sep='\\t')\n",
    "test = pd.read_csv('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/testwithlabels.txt', sep='\\t')\n",
    "all_sets = [train, dev, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7ecafbf11806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_emoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_or_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_or_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_or_emo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def other_or_emo(emotion):\n",
    "    emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "    return 0 if emotion2label[emotion]==0 else 1\n",
    "\n",
    "def remove_emoji(text):\n",
    "    return ''.join([char for char in list(text) if not is_emoji(char)]).strip()\n",
    "\n",
    "train.label = train.label.apply(other_or_emo)\n",
    "dev.label = dev.label.apply(other_or_emo)\n",
    "test.label = test.label.apply(other_or_emo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData2(dir):\n",
    "    df = pd.read_csv(dir, sep='\\t')\n",
    "\n",
    "    df.label = df.label.apply(other_or_emo)\n",
    "    labels = df.label.tolist()\n",
    "\n",
    "    df['t1_len'] = df['turn1'].apply(count_length)\n",
    "    df['t2_len'] = df['turn2'].apply(count_length)\n",
    "    df['t3_len'] = df['turn3'].apply(count_length)\n",
    "\n",
    "    df['t1_upper'] = df['turn1'].apply(count_upper_case)\n",
    "    df['t2_upper'] = df['turn2'].apply(count_upper_case)\n",
    "    df['t3_upper'] = df['turn3'].apply(count_upper_case)\n",
    "    \n",
    "    df['t1_smil'] = df['turn1'].apply(str2emoji).apply(lambda x: sum([1 for s in x if is_emoji(s)]))\n",
    "    df['t2_smil'] = df['turn2'].apply(str2emoji).apply(lambda x: sum([1 for s in x if is_emoji(s)]))\n",
    "    df['t3_smil'] = df['turn3'].apply(str2emoji).apply(lambda x: sum([1 for s in x if is_emoji(s)]))\n",
    "    \n",
    "    df['turn1'] = df['turn1'].apply(str2emoji).apply(remove_emoji).apply(lambda x: x.lower()).apply(preprocessString)\n",
    "    df['turn2'] = df['turn2'].apply(str2emoji).apply(remove_emoji).apply(lambda x: x.lower()).apply(preprocessString)\n",
    "    df['turn3'] = df['turn3'].apply(str2emoji).apply(remove_emoji).apply(lambda x: x.lower()).apply(preprocessString)\n",
    "\n",
    "    ind = df.id.tolist()\n",
    "\n",
    "    t1, t2, t3 = np.asarray(df.turn1), np.asarray(df.turn2), np.asarray(df.turn3)\n",
    "    len1, len2, len3 = np.asarray(df.t1_len), np.asarray(df.t2_len), np.asarray(df.t3_len)\n",
    "    smil1, smil2, smil3 = np.asarray(df.t1_smil), np.asarray(df.t2_smil), np.asarray(df.t3_smil)\n",
    "    case1, case2, case3 = np.asarray(df.t1_upper), np.asarray(df.t2_upper), np.asarray(df.t3_upper)\n",
    "    \n",
    "    return ind, labels, t1, t2, t3, len1, len2, len3, case1, case2, case3, smil1, smil2, smil3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens...\n",
      "Pad everything\n"
     ]
    }
   ],
   "source": [
    "validationDataPath = '/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/dev.txt'\n",
    "testDataPath = '/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/testwithlabels.txt'\n",
    "trainDataPath = '/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/starterkitdata/train.txt'\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "ind, labels, t1, t2, t3, len1, len2, len3, case1, case2, case3, smil1, smil2, smil3 = preprocessData2(trainDataPath)\n",
    "ind_val, validationLabels, t1_val, t2_val, t3_val, len1_val, len2_val, len3_val, case1_val, case2_val, case3_val, smil1_val, smil2_val, smil3_val = preprocessData2(validationDataPath)\n",
    "ind_test, testLabels, t1_test, t2_test, t3_test, len1_test, len2_test, len3_test, case1_test, case2_test, case3_test, smil1_test, smil2_test, smil3_test = preprocessData2(testDataPath)\n",
    "\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(t1+t2+t3)\n",
    "wordIndex = tokenizer.word_index\n",
    "\n",
    "t1, t2, t3 = tokenizer.texts_to_sequences(t1), tokenizer.texts_to_sequences(t2), tokenizer.texts_to_sequences(t3)\n",
    "t1_val, t2_val, t3_val = tokenizer.texts_to_sequences(t1_val), tokenizer.texts_to_sequences(t2_val), tokenizer.texts_to_sequences(t3_val)\n",
    "t1_test, t2_test, t3_test = tokenizer.texts_to_sequences(t1_test), tokenizer.texts_to_sequences(t2_test), tokenizer.texts_to_sequences(t3_test)\n",
    "\n",
    "print(\"Pad everything\")\n",
    "t1, t2, t3 = pad_sequences(t1, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "t1_val, t2_val, t3_val = pad_sequences(t1_val, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2_val, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "t1_test, t2_test, t3_test = pad_sequences(t1_test, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2_test, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "validationLabels = to_categorical(np.asarray(validationLabels))\n",
    "testLabels = to_categorical(np.asarray(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(ind)\n",
    "t1 = t1[ind]\n",
    "t2 = t2[ind]\n",
    "t3 = t3[ind]\n",
    "case1 = np.asarray([case1[i] for i in ind])\n",
    "case2 = np.asarray([case2[i] for i in ind])\n",
    "case3 = np.asarray([case3[i] for i in ind])\n",
    "len1 = np.asarray([len1[i] for i in ind])\n",
    "len2 = np.asarray([len2[i] for i in ind])\n",
    "len3 = np.asarray([len3[i] for i in ind])\n",
    "smil1 = np.asarray([smil1[i] for i in ind])\n",
    "smil2 = np.asarray([smil2[i] for i in ind])\n",
    "smil3 = np.asarray([smil3[i] for i in ind])\n",
    "labels = labels[ind]\n",
    "metrics = {\"accuracy\" : [], \"microPrecision\" : [], \"microRecall\" : [], \"microF1\" : []}\n",
    "embMat = np.zeros((20000, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(embeddingMatrix):\n",
    "    \"\"\"Constructs the architecture of the model\n",
    "    Input:\n",
    "        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\n",
    "    Output:\n",
    "        model : three layer lstm model with one layer of meta data\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = Input(shape=(100,), dtype='int32', name='turn_1')\n",
    "    t2 = Input(shape=(100,), dtype='int32', name='turn_2')\n",
    "    t3 = Input(shape=(100,), dtype='int32', name='turn_3')\n",
    "\n",
    "    meta_data = Input(shape=(9,), dtype='float32', name='meta_input')\n",
    "    \n",
    "    ########## mata data layer #############\n",
    "    hidden_layer = Dense(32, activation='relu')\n",
    "    meta_layer = hidden_layer(meta_data)\n",
    "    ########################################\n",
    "\n",
    "    ########## Conversation layer ##########\n",
    "    twitterEmbeddings = Embedding(embeddingMatrix.shape[0], EMBEDDING_DIM, weights=[embeddingMatrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "\n",
    "    emb1 = twitterEmbeddings(t1)\n",
    "    emb2 = twitterEmbeddings(t2)\n",
    "    emb3 = twitterEmbeddings(t3)\n",
    "\n",
    "    #LSTM layers, need to define a new one for different embeddings\n",
    "    lstm = Bidirectional(LSTM(32, dropout=DROPOUT, return_sequences=True))\n",
    "    second_layer = LSTM(64, dropout=DROPOUT)\n",
    "\n",
    "    lstm1 = lstm(emb1)\n",
    "    lstm2 = lstm(emb2)\n",
    "    lstm3 = lstm(emb3)\n",
    "\n",
    "    concatenated_lstm = Concatenate(axis=-1)([lstm1, lstm2, lstm3])\n",
    "    concatenated_lstm = Dropout(DROPOUT)(concatenated_lstm)\n",
    "    concatenated_lstm = second_layer(concatenated_lstm)\n",
    "    concatenated_lstm = Dropout(DROPOUT)(concatenated_lstm)\n",
    "    \n",
    "    ############ Merging ############\n",
    "    merge_out = Concatenate(axis=-1)([concatenated_lstm, meta_layer])\n",
    "\n",
    "    #output\n",
    "    model_output = Dense(2, activation='sigmoid')(merge_out)\n",
    "    model_output = Dense()\n",
    "    model = Model([t1, t2, t3, meta_data], model_output)\n",
    "\n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    adam =  optimizers.adam(lr=LEARNING_RATE)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up mo.. mod... mod..EL.. MODEL MOOODEEEL\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "turn_1 (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "turn_2 (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "turn_3 (InputLayer)             (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 200)     4000000     turn_1[0][0]                     \n",
      "                                                                 turn_2[0][0]                     \n",
      "                                                                 turn_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100, 64)      59648       embedding_4[0][0]                \n",
      "                                                                 embedding_4[1][0]                \n",
      "                                                                 embedding_4[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 100, 192)     0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "                                                                 bidirectional_4[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 100, 192)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 64)           65792       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "meta_input (InputLayer)         (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           320         meta_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           dropout_8[0][0]                  \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            194         concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,125,954\n",
      "Trainable params: 4,125,954\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2322\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2324\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'bidirectional_4_2/while_1/TensorArrayReadV3' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2326\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'bidirectional_4_2/while_1/TensorArrayReadV3' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6d63a87cb439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membMat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    508\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    510\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    511\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m     \"\"\"\n\u001b[0;32m-> 2757\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 630\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 814\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    815\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 814\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    815\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\u001b[0m in \u001b[0;36m_TensorArrayReadGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    103\u001b[0m   g = (tensor_array_ops.TensorArray(dtype=dtype, handle=handle, flow=flow,\n\u001b[1;32m    104\u001b[0m                                     colocate_with_first_write_call=False)\n\u001b[0;32m--> 105\u001b[0;31m        .grad(source=grad_source, flow=flow))\n\u001b[0m\u001b[1;32m    106\u001b[0m   \u001b[0mw_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, source, flow, name)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, source, flow, name)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorArrayGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         g_handle, unused_flow = gen_data_flow_ops.tensor_array_grad_v3(\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6023\u001b[0m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6024\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6025\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6026\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6027\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5225\u001b[0m         default.building_function, default.as_default)\n\u001b[1;32m   5226\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5227\u001b[0;31m       with super(_DefaultGraphStack, self).get_controller(\n\u001b[0m\u001b[1;32m   5228\u001b[0m           default) as g, context.graph_mode():\n\u001b[1;32m   5229\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Set up mo.. mod... mod..EL.. MODEL MOOODEEEL\")\n",
    "\n",
    "NUM_EPOCHS=30\n",
    "LEARNING_RATE=0.005\n",
    "LSTM_DIM = 128\n",
    "BATCH_SIZE = 200\n",
    "EMBEDDING_DIM = 200\n",
    "DROPOUT = 0.3\n",
    "\n",
    "meta_data = np.asarray([len1, len2, len3, case1, case2, case3, smil1, smil2, smil3]).T\n",
    "meta_data_val = np.asarray([len1_val, len2_val, len3_val, case1_val, case2_val, case3_val, smil1_val, smil2_val, smil3_val]).T\n",
    "meta_data_test = np.asarray([len1_test, len2_test, len3_test, case1_test, case2_test, case3_test, smil1_test, smil2_test, smil3_test]).T\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "mc = ModelCheckpoint('EP%d_LR%de-5_LDim%d_BS%d.h5'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "model = buildModel(embMat)\n",
    "model.summary()\n",
    "history = model.fit([t1, t2, t3, meta_data], labels, validation_data=([t1_val, t2_val, t3_val, meta_data_val], validationLabels), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=2, callbacks=[es, mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

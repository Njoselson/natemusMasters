{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test/train data for first model...\n",
      "Extracting tokens...\n",
      "Right format...\n",
      "Preprocess for second model...\n",
      "Extracting tokens for second model...\n",
      "Load models...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(7)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, Concatenate, Dropout, Bidirectional, Reshape, Flatten\n",
    "from keras import optimizers\n",
    "from keras.models import load_model, Model\n",
    "from keras import callbacks\n",
    "from matplotlib import pyplot\n",
    "import emoji\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from helper_functions import *\n",
    "from bidir_model import *\n",
    "\n",
    "#get all test data\n",
    "print(\"Processing test/train data for first model...\")\n",
    "trainIndices, trainTexts, labels, u1_train, u2_train, u3_train, smil_train = preprocessData(trainDataPath, mode=\"train\")\n",
    "validationIndices, validationTexts, validationLabels, u1_val, u2_val, u3_val, smil_val = preprocessData(validationDataPath, mode=\"train\")\n",
    "testIndices, testTexts, testLabels, u1_test, u2_test, u3_test, smil_test = preprocessData(testDataPath, mode=\"train\")\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(u1_train+u2_train+u3_train)\n",
    "\n",
    "print(\"Right format...\")\n",
    "u1_trainSequences, u2_trainSequences, u3_trainSequences, smil_trainSeq = tokenizer.texts_to_sequences(u1_train), tokenizer.texts_to_sequences(u2_train), tokenizer.texts_to_sequences(u3_train), tokenizer.texts_to_sequences(smil_train)\n",
    "u1_testSequences, u2_testSequences, u3_testSequences, smil_testSeq = tokenizer.texts_to_sequences(u1_test), tokenizer.texts_to_sequences(u2_test), tokenizer.texts_to_sequences(u3_test), tokenizer.texts_to_sequences(smil_test)\n",
    "u1_valSequences, u2_valSequences, u3_valSequences, smil_valSeq = tokenizer.texts_to_sequences(u1_val), tokenizer.texts_to_sequences(u2_val), tokenizer.texts_to_sequences(u3_val), tokenizer.texts_to_sequences(smil_val)\n",
    "u1_data = pad_sequences(u1_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u2_data = pad_sequences(u2_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u3_data = pad_sequences(u3_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "smil_data = pad_sequences(smil_trainSeq, maxlen=20)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u1_valData = pad_sequences(u1_valSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u2_valData = pad_sequences(u2_valSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u3_valData = pad_sequences(u3_valSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "smil_valData = pad_sequences(smil_valSeq, maxlen=20)\n",
    "validationLabels = to_categorical(np.asarray(validationLabels))\n",
    "u1_testData = pad_sequences(u1_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u2_testData = pad_sequences(u2_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "u3_testData = pad_sequences(u3_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "smil_testData = pad_sequences(smil_testSeq, maxlen=20)\n",
    "testLabels = to_categorical(np.asarray(testLabels))\n",
    "    \n",
    "print(\"Preprocess for second model...\")\n",
    "trainIndices, labels2, t1, t2, t3, len1, len2, len3, case1, case2, case3, smil1, smil2, smil3 = preprocessData2(trainDataPath)\n",
    "ind_val, labels2_val, t1_val, t2_val, t3_val, len1_val, len2_val, len3_val, case1_val, case2_val, case3_val, smil1_val, smil2_val, smil3_val = preprocessData2(validationDataPath)\n",
    "ind_test, labels2_test, t1_test, t2_test, t3_test, len1_test, len2_test, len3_test, case1_test, case2_test, case3_test, smil1_test, smil2_test, smil3_test = preprocessData2(testDataPath)\n",
    "\n",
    "print(\"Extracting tokens for second model...\")\n",
    "tokenizerModelTwo = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizerModelTwo.fit_on_texts(t1+t2+t3)\n",
    "wordIndexModelTwo = tokenizerModelTwo.word_index\n",
    "\n",
    "t1, t2, t3 = tokenizerModelTwo.texts_to_sequences(t1), tokenizerModelTwo.texts_to_sequences(t2), tokenizerModelTwo.texts_to_sequences(t3)\n",
    "t1_val, t2_val, t3_val = tokenizerModelTwo.texts_to_sequences(t1_val), tokenizerModelTwo.texts_to_sequences(t2_val), tokenizerModelTwo.texts_to_sequences(t3_val)\n",
    "t1_test, t2_test, t3_test = tokenizerModelTwo.texts_to_sequences(t1_test), tokenizerModelTwo.texts_to_sequences(t2_test), tokenizerModelTwo.texts_to_sequences(t3_test)\n",
    "\n",
    "t1, t2, t3 = pad_sequences(t1, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "t1_val, t2_val, t3_val = pad_sequences(t1_val, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2_val, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "t1_test, t2_test, t3_test = pad_sequences(t1_test, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t2_test, maxlen=MAX_SEQUENCE_LENGTH), pad_sequences(t3_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "meta_data = np.asarray([len1, len2, len3, case1, case2, case3, smil1, smil2, smil3]).T\n",
    "meta_data_val = np.asarray([len1_val, len2_val, len3_val, case1_val, case2_val, case3_val, smil1_val, smil2_val, smil3_val]).T\n",
    "meta_data_test = np.asarray([len1_test, len2_test, len3_test, case1_test, case2_test, case3_test, smil1_test, smil2_test, smil3_test]).T\n",
    "metrics = {\"accuracy\" : [], \"microPrecision\" : [], \"microRecall\" : [], \"microF1\" : []}\n",
    "\n",
    "#Load models\n",
    "print(\"Load models...\")\n",
    "model1 = load_model('EP100_LR100e-5_LDim128_BS2500.h5')\n",
    "model2 = load_model('../b_smileyZeroEmbeddings_F1_73/EP2_LR100e-5_LDim128_BS200.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and val data preds...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and val data preds...\")\n",
    "preds1_train = model1.predict([t1, t2, t3, meta_data], batch_size=BATCH_SIZE)\n",
    "preds2_train = model2.predict([u1_data,u2_data,u3_data, smil_data], batch_size=BATCH_SIZE)\n",
    "preds1_val = model1.predict([t1_val, t2_val, t3_val, meta_data_val], batch_size=BATCH_SIZE)\n",
    "preds2_val = model2.predict([u1_valData,u2_valData,u3_valData, smil_valData], batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Make predictions...\")\n",
    "preds1 = model1.predict([t1_test, t2_test, t3_test, meta_data_test], batch_size=BATCH_SIZE)\n",
    "preds2 = model2.predict([u1_testData, u2_testData, u3_testData, smil_testData], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_to_other(preds):\n",
    "    recoded_preds = np.zeros((len(preds),2))\n",
    "    for row in range(0,len(preds)):\n",
    "        if (np.argmax(preds[row,:]) == 0):\n",
    "            recoded_preds[row,0] = 1\n",
    "        else:\n",
    "            recoded_preds[row,1] = 1\n",
    "    return recoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 1.2807 - acc: 0.5677 - val_loss: 1.0492 - val_acc: 0.8211\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04923, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 1.1371 - acc: 0.6726 - val_loss: 0.9055 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04923 to 0.90552, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.9981 - acc: 0.7187 - val_loss: 0.7716 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90552 to 0.77159, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.8664 - acc: 0.8370 - val_loss: 0.6571 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.77159 to 0.65711, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7517 - acc: 0.8703 - val_loss: 0.5682 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.65711 to 0.56820, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6582 - acc: 0.8747 - val_loss: 0.5035 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56820 to 0.50352, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5847 - acc: 0.8784 - val_loss: 0.4581 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50352 to 0.45806, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5279 - acc: 0.8826 - val_loss: 0.4258 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.45806 to 0.42579, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.4842 - acc: 0.8852 - val_loss: 0.4041 - val_acc: 0.8973\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42579 to 0.40408, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.4502 - acc: 0.8887 - val_loss: 0.3891 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.40408 to 0.38909, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.4238 - acc: 0.8914 - val_loss: 0.3786 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.38909 to 0.37860, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.4029 - acc: 0.8933 - val_loss: 0.3707 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.37860 to 0.37072, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.3863 - acc: 0.8946 - val_loss: 0.3649 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.37072 to 0.36487, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.3729 - acc: 0.8958 - val_loss: 0.3613 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.36487 to 0.36131, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.3620 - acc: 0.8974 - val_loss: 0.3585 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.36131 to 0.35849, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.3531 - acc: 0.8987 - val_loss: 0.3558 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.35849 to 0.35579, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.3457 - acc: 0.8995 - val_loss: 0.3537 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.35579 to 0.35370, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.3396 - acc: 0.9007 - val_loss: 0.3522 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.35370 to 0.35217, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.3344 - acc: 0.9010 - val_loss: 0.3513 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.35217 to 0.35126, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.3301 - acc: 0.9016 - val_loss: 0.3495 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.35126 to 0.34950, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.3263 - acc: 0.9017 - val_loss: 0.3483 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.34950 to 0.34825, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.3232 - acc: 0.9024 - val_loss: 0.3473 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.34825 to 0.34735, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.3204 - acc: 0.9027 - val_loss: 0.3464 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.34735 to 0.34642, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.3181 - acc: 0.9028 - val_loss: 0.3470 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34642\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.3160 - acc: 0.9032 - val_loss: 0.3440 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.34642 to 0.34398, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.3143 - acc: 0.9029 - val_loss: 0.3444 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.34398\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.3127 - acc: 0.9031 - val_loss: 0.3441 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.34398\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.3113 - acc: 0.9030 - val_loss: 0.3439 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.34398 to 0.34391, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.3101 - acc: 0.9032 - val_loss: 0.3446 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.34391\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.3091 - acc: 0.9031 - val_loss: 0.3442 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.34391\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.3081 - acc: 0.9033 - val_loss: 0.3430 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.34391 to 0.34303, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3073 - acc: 0.9035 - val_loss: 0.3422 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.34303 to 0.34215, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3065 - acc: 0.9032 - val_loss: 0.3428 - val_acc: 0.8962\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.34215\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3058 - acc: 0.9035 - val_loss: 0.3418 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.34215 to 0.34182, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3052 - acc: 0.9034 - val_loss: 0.3421 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.34182\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3047 - acc: 0.9036 - val_loss: 0.3418 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.34182 to 0.34179, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3042 - acc: 0.9035 - val_loss: 0.3433 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.34179\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3037 - acc: 0.9037 - val_loss: 0.3417 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.34179 to 0.34165, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3033 - acc: 0.9033 - val_loss: 0.3410 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.34165 to 0.34104, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.3029 - acc: 0.9033 - val_loss: 0.3413 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.34104\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.3025 - acc: 0.9035 - val_loss: 0.3405 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.34104 to 0.34047, saving model to EP2_LR100e-5_LDim128_BS200.h5\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.3021 - acc: 0.9035 - val_loss: 0.3427 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.34047\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.3018 - acc: 0.9035 - val_loss: 0.3424 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.34047\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.3015 - acc: 0.9037 - val_loss: 0.3408 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.34047\n",
      "Epoch 00044: early stopping\n"
     ]
    }
   ],
   "source": [
    "def buildModel():\n",
    "    inp = Input(shape=(6,), dtype='float32')\n",
    "    out = Dense(4, activation='sigmoid')(inp)\n",
    "    model = Model([inp], out)\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = buildModel()\n",
    "train_data = np.concatenate((preds1_train, preds2_train), axis=1)\n",
    "val_data = np.concatenate((preds1_val, preds2_val), axis=1)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('EP%d_LR%de-5_LDim%d_BS%d.h5'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE), monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "# fit model\n",
    "history = model.fit(train_data, labels, validation_data=(val_data, validationLabels), epochs=100, batch_size=BATCH_SIZE, verbose=2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [4275.  194.  204.  251.]\n",
      "False Positives per class :  [151. 119. 123. 192.]\n",
      "False Negatives per class :  [402.  90.  46.  47.]\n",
      "Class happy : Precision : 0.620, Recall : 0.683, F1 : 0.650\n",
      "Class sad : Precision : 0.624, Recall : 0.816, F1 : 0.707\n",
      "Class angry : Precision : 0.567, Recall : 0.842, F1 : 0.677\n",
      "Ignoring the Others class, Macro Precision : 0.6034, Macro Recall : 0.7805, Macro F1 : 0.6806\n",
      "Ignoring the Others class, Micro TP : 649, FP : 434, FN : 183\n",
      "Accuracy : 0.8938, Micro Precision : 0.5993, Micro Recall : 0.7800, Micro F1 : 0.6778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8938101288800145, 0.5992613, 0.7800481, 0.6778067400289556)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_best = load_model('/Users/rasmushallen/Desktop/master_thesis_code/natemusMasters/b_LSTM_F1_73/b_smileyZeroEmbeddings_F1_73/EP2_LR100e-5_LDim128_BS200.h5')\n",
    "#best_pred = model_best.predict([u1_testData, u2_testData, u3_testData, smil_testData], batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = np.concatenate((preds1, preds2), axis=1)\n",
    "predictions = model.predict(test_data)\n",
    "getMetrics(predictions, testLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
